# -*- coding: utf-8 -*-
"""Flight Fare Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ax7bhqdKlvU4e-A95sgTTjaljEXHncU7
"""

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
data=pd.read_excel('/content/Data_Train.xlsx')
data.head()

data.shape

data.columns

data.isnull().sum()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd
import seaborn as sns
import re,nltk,json
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras import models
from keras import layers
from tensorflow.keras.layers import LSTM,GRU
from tensorflow.keras.models import load_model
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report 
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score
from sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
np.random.seed(42)
class color: # Text style
   PURPLE = '\033[95m'
   CYAN = '\033[96m'
   DARKCYAN = '\033[36m'
   BLUE = '\033[94m'
   GREEN = '\033[92m'
   YELLOW = '\033[93m'
   RED = '\033[91m'
   BOLD = '\033[1m'
   UNDERLINE = '\033[4m'
   END = '\033[0m'

data['Price'].value_counts()

# Cleaning Data [Remove unncessary symbols]
def cleaning_data(row):
      headlines = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
      return headlines
# Apply the function into the dataframe
data['cleaned'] = data['Airline'].apply(cleaning_data)  

# print some cleaned reviews from the dataset
sample_data = [20,40,50,55,70,90,95,100,110]
for i in sample_data:
  print('Original: ',data.Airline[i],'\nCleaned:',
           data.cleaned[i],'\n','Category:-- ',data.Price[i],'\n')

# Cleaning Data [Remove unncessary symbols]
def cleaning_data(row):
      headlines = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
      return headlines
# Apply the function into the dataframe
data['cleaned'] = data['Date_of_Journey'].apply(cleaning_data)  

# print some cleaned reviews from the dataset
sample_data = [20,40,50,55,70,90,95,100,110]
for i in sample_data:
  print('Original: ',data.Date_of_Journey[i],'\nCleaned:',
           data.cleaned[i],'\n','Category:-- ',data.Price[i],'\n')

# Cleaning Data [Remove unncessary symbols]
def cleaning_data(row):
      headlines = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
      return headlines
# Apply the function into the dataframe
data['cleaned'] = data['Source'].apply(cleaning_data)  

# print some cleaned reviews from the dataset
sample_data = [20,40,50,55,70,90,95,100,110]
for i in sample_data:
  print('Original: ',data.Source[i],'\nCleaned:',
           data.cleaned[i],'\n','Category:-- ',data.Price[i],'\n')

# Cleaning Data [Remove unncessary symbols]
def cleaning_data(row):
      headlines = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
      return headlines
# Apply the function into the dataframe
data['cleaned'] = data['Destination'].apply(cleaning_data)  

# print some cleaned reviews from the dataset
sample_data = [20,40,50,55,70,90,95,100,110]
for i in sample_data:
  print('Original: ',data.Destination[i],'\nCleaned:',
           data.cleaned[i],'\n','Category:-- ',data.Price[i],'\n')

# Cleaning Data [Remove unncessary symbols]
def cleaning_data(row):
      headlines = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
      return headlines
# Apply the function into the dataframe
data['cleaned'] = data['Route'].apply(cleaning_data)  

# print some cleaned reviews from the dataset
sample_data = [20,40,50,55,70,90,95,100,110]
for i in sample_data:
  print('Original: ',data.Route[i],'\nCleaned:',
           data.cleaned[i],'\n','Category:-- ',data.Price[i],'\n')

# Cleaning Data [Remove unncessary symbols]
def cleaning_data(row):
      headlines = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
      return headlines
# Apply the function into the dataframe
data['cleaned'] = data['Dep_Time'].apply(cleaning_data)  

# print some cleaned reviews from the dataset
sample_data = [20,40,50,55,70,90,95,100,110]
for i in sample_data:
  print('Original: ',data.Dep_Time[i],'\nCleaned:',
           data.cleaned[i],'\n','Category:-- ',data.Price[i],'\n')

# Cleaning Data [Remove unncessary symbols]
def cleaning_data(row):
      headlines = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
      return headlines
# Apply the function into the dataframe
data['cleaned'] = data['Arrival_Time'].apply(cleaning_data)  

# print some cleaned reviews from the dataset
sample_data = [20,40,50,55,70,90,95,100,110]
for i in sample_data:
  print('Original: ',data.Arrival_Time[i],'\nCleaned:',
           data.cleaned[i],'\n','Category:-- ',data.Price[i],'\n')

# Cleaning Data [Remove unncessary symbols]
def cleaning_data(row):
      headlines = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
      return headlines
# Apply the function into the dataframe
data['cleaned'] = data['Duration'].apply(cleaning_data)  

# print some cleaned reviews from the dataset
sample_data = [20,40,50,55,70,90,95,100,110]
for i in sample_data:
  print('Original: ',data.Duration[i],'\nCleaned:',
           data.cleaned[i],'\n','Category:-- ',data.Price[i],'\n')

# Cleaning Data [Remove unncessary symbols]
def cleaning_data(row):
      headlines = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
      return headlines
# Apply the function into the dataframe
data['cleaned'] = data['Total_Stops'].apply(cleaning_data)  

# print some cleaned reviews from the dataset
sample_data = [20,40,50,55,70,90,95,100,110]
for i in sample_data:
  print('Original: ',data.Total_Stops[i],'\nCleaned:',
           data.cleaned[i],'\n','Category:-- ',data.Price[i],'\n')

# Cleaning Data [Remove unncessary symbols]
def cleaning_data(row):
      headlines = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
      return headlines
# Apply the function into the dataframe
data['cleaned'] = data['Additional_Info'].apply(cleaning_data)  

# print some cleaned reviews from the dataset
sample_data = [20,40,50,55,70,90,95,100,110]
for i in sample_data:
  print('Original: ',data.Additional_Info[i],'\nCleaned:',
           data.cleaned[i],'\n','Category:-- ',data.Price[i],'\n')

data.info()

def data_summary(data):
    
    """
  
    
    Args:
        dataset: list of cleaned sentences   
        
    Returns:
        Number of documnets per class: int 
        Number of words per class: int
        Number of unique words per class: int
    """
    documents = []
    words = []
    u_words = []
    total_u_words = [word.strip().lower() for t in list(data.cleaned) for word in t.strip().split()]
    class_label= [k for k,v in data.Price.value_counts().to_dict().items()]
  # find word list
    for label in class_label: 
        word_list = [word.strip().lower() for t in list(data[data.Price==label].cleaned) for word in t.strip().split()]
        counts = dict()
        for word in word_list:
                counts[word] = counts.get(word, 0)+1
        # sort the dictionary of word list  
        ordered = sorted(counts.items(), key= lambda item: item[1],reverse = True)
        # Documents per class
        documents.append(len(list(data[data.Price==label].cleaned)))
        # Total Word per class
        words.append(len(word_list))
        # Unique words per class 
        u_words.append(len(np.unique(word_list)))
       
        print("\nClass Name : ",label)
        print("Number of Documents:{}".format(len(list(data[data.Price==label].cleaned))))  
        print("Number of Words:{}".format(len(word_list))) 
        print("Number of Unique Words:{}".format(len(np.unique(word_list)))) 
        print("Most Frequent Words:\n")
        for k,v in ordered[:10]:
              print("{}\t{}".format(k,v))
    print("Total Number of Unique Words:{}".format(len(np.unique(total_u_words))))           
   
    return documents,words,u_words,class_label

#call the fucntion
documents,words,u_words,class_names = data_summary(data)

#==================================================
                                       ################# Label Encoding Function #########
                                       #==================================================

def label_encoding(category,bool):
    """
    This function will return the encoded labels in array format. 
    
    Args:
        category: series of class names(str)
        bool: boolean (True or False)
        
    Returns:
        labels: numpy array 
    """
    le = LabelEncoder()
    le.fit(category)
    encoded_labels = le.transform(category)
    labels = np.array(encoded_labels) # Converting into numpy array
    class_names =le.classes_ ## Define the class names again
    if bool == True:
        print("\n\t\t\t===== Label Encoding =====","\nClass Names:-->",le.classes_)
        for i in sample_data:
            print(category[i],' ', encoded_labels[i],'\n')

    return labels



                           #===========================================================
                           ################# Dataset Splitting Function ###############
                           #=========================================================== 

def dataset_split(headlines,category):
    """
    This function will return the splitted (90%-10%-10%) feature vector . 
    
    Args:
        headlines: sequenced headlines 
        category: encoded lables (array) 
        
    Returns:
        X_train: training data 
        X_valid: validation data
        X_test : testing feature vector 
        y_train: training encoded labels (array) 
        y_valid: training encoded labels (array) 
        y_test : testing encoded labels (array) 
    """

    X,X_test,y,y_test = train_test_split(headlines,category,test_size = 0.1,random_state =1,shuffle=False)
    X_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size = 0.2,random_state =1,shuffle=False)
    print(color.BOLD+"\nDataset Distribution:\n"+color.END)
    print("\tSet Name","\t\tSize")
    print("\t========\t\t======")

    print("\tFull\t\t\t",len(headlines),
        "\n\tTraining\t\t",len(X_train),
        "\n\tTest\t\t\t",len(X_test),
        "\n\tValidation\t\t",len(X_valid))
  
    return X_train,X_valid,X_test,y_train,y_valid,y_test

labels = label_encoding(data.Price,True)

#data['mer'] = data[['Airline', 'Date_of_Journey', 'Source', 'Destination', 'Route',
#       'Dep_Time', 'Arrival_Time', 'Duration', 'Total_Stops',
#       'Additional_Info',]].agg(' '.join, axis=1)
#data

data['mer']=data[['Airline','Date_of_Journey','Source','Destination']].agg(' '.join,axis=1)
data

X_train,X_valid,X_test,y_train,y_valid,y_test = dataset_split(data.mer,labels)

vocab_size = 5700
embedding_dim = 128
max_length = 150
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"

def padded_headlines(original,encoded,padded):
  '''
  print the samples padded headlines
  '''
  print(color.BOLD+"\n\t\t\t====== Encoded Sequences ======"+color.END,"\n")  
  print(original,"\n",encoded) 
  print(color.BOLD+"\n\t\t\t====== Paded Sequences ======\n"+color.END,original,"\n",padded)

# Train Data Tokenization
tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index
train_sequences = tokenizer.texts_to_sequences(X_train)
train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)

#============================== Tokenizer Info =================================
(word_counts,word_docs,word_index,document_count) = (tokenizer.word_counts,
                                                       tokenizer.word_docs,
                                                       tokenizer.word_index,
                                                       tokenizer.document_count)
def tokenizer_info(mylist,bool):
  ordered = sorted(mylist.items(), key= lambda item: item[1],reverse = bool)
  for w,c in ordered[:10]:
    print(w,"\t",c)
  #=============================== Print all the information =========================
print(color.BOLD+"\t\t\t====== Tokenizer Info ======"+color.END)   
print("Words --> Counts:")
tokenizer_info(word_counts,bool =True )
print("\nWords --> Documents:")
tokenizer_info(word_docs,bool =True )
print("\nWords --> Index:")
tokenizer_info(word_index,bool =True )    
print("\nTotal Documents -->",document_count)
print(f"Found {len(word_index)} unique tokens")

padded_headlines(X_train[20],train_sequences[20],train_padded[20])

# Validation Data Tokenization
validation_sequences = tokenizer.texts_to_sequences(X_valid)
validation_padded = pad_sequences(validation_sequences, padding=padding_type , maxlen=max_length)
#padded_headlines(X_valid[1150],validation_sequences[1],validation_padded[1])

# Test Data Tokenization
test_sequences = tokenizer.texts_to_sequences(X_test)
test_padded = pad_sequences(test_sequences, padding=padding_type , maxlen=max_length)
#padded_headlines(X_test[800],test_sequences[800],test_padded[800])

# Labels Tokenization
#label_tokenizer = Tokenizer()
#label_tokenizer.fit_on_texts(dataset.category)

train_label_seq = y_train
valid_label_seq = y_valid
testing_label_seq = y_test

#print(train_label_seq.shape)
#print(valid_label_seq.shape)
#print(testing_label_seq.shape)

from sklearn import tree
import sys
import matplotlib.pyplot as plt
def opt(particles,iterations,dimensions,inertia):
  max_d=40
  min_d=10
  max_f=1e-5
  min_f=1e-8
  max_id=1e-4
  min_id=1e-1
  max_l=0.5
  min_l=0.1
  x=np.random.rand(particles,1)*(max_d - min_d)+min_d
  y=np.random.rand(particles,1)*(max_f - min_f)+min_f
  z=np.random.rand(particles,1)*(max_id - min_id)+min_id
  q=np.random.rand(particles,1)*(max_l - min_l)+min_l
  c=np.concatenate((x,y,z,q),axis=1)
  v=np.zeros((particles,dimensions))
  c1=2
  c2=2
  p_best=np.zeros((particles,dimensions))
  p_best_val=np.zeros(particles)+sys.maxsize
  g_best=np.zeros(dimensions)
  g_best_val=sys.maxsize
  best_iter=np.zeros(iterations)
  p_best_RGS=np.empty((particles),dtype=object)
  g_best_RGS=sys.maxsize
  #plot(c)
  from sklearn.metrics import mean_squared_error
  for i in range(iterations):
    for j in range(particles):
      rgs=tree.DecisionTreeRegressor(max_depth=c[j][0],max_features=c[j][1],min_impurity_decrease=c[j][2],
                                     min_samples_leaf=c[j][3])
      rgs.fit(train_padded,train_label_seq)
      y_predict=rgs.predict(test_padded)
      mse=mean_squared_error(testing_label_seq,y_predict)

      if(mse<p_best_val[j]):
        p_best_val[j]=mse
        p_best_RGS[j]=rgs
        p_best[j]=c[j].copy()

      aux=np.argmin(p_best_val)
      if(p_best_val[aux] < g_best_val):
        g_best_val=p_best_val[aux]
        g_best=p_best[aux].copy()
        g_best_RGS=p_best_RGS[aux]

      rand1=np.random.random()
      rand2=np.random.random()

      w=inertia
      
      v[j]=w*v[j]+c1*(p_best[j]-c[j])*rand1+c2*(g_best-c[j])*rand2
      c[j]=c[j]+v[j]

      if(c[j][3] <min_l):
        c[j][3]=min_l
      if(c[j][3] >max_l):
        c[j][3]=max_l
      if(c[j][2] <min_id):
        c[j][2]=min_id
      if(c[j][2] >max_id):
        c[j][2]=max_id
      if(c[j][1]<min_f):
        c[j][1]=min_f
      if(c[j][1]>max_f):
        c[j][1]=max_f
      if(c[j][0]<min_d):
        c[j][0]=min_d
      if(c[j][0]>max_d):
        c[j][0]=max_d
      
    best_iter[i]=g_best_val

    print('best value iteration %d = %f\n' %(i,g_best_val))

  print('group best configuration found')
  print(g_best)
  print('\n')
  print('best regressor \n')
  print(g_best_RGS)
  print('\n')

  t=range(iterations)
  plt.plot(t,best_iter,label='Fitness Value')
  plt.legend()
  plt.show()
  #plot(c)

  predict_test=g_best_RGS.predict(test_padded)
  #print(color.BOLD + "predictions with the population best value found:\n" + color.END)
  evaluate(predict_test)

def evaluate(predictions):
  from sklearn.metrics import mean_squared_error
  import statistics as st
  predict_test=predictions

  plt.plot(range(len(y_test)),y_test,label='Real')
  plt.plot(range(len(predict_test)),predict_test,label='Predicted')
  plt.legend()
  plt.show()
  
  mse=np.sqrt(mean_squared_error(y_test,predict_test))
  print("\n")
  print("Mean Squared_error: \t %f "%mse)
  print("\n")
  print('prediction average: \t %f'%((predict_test.sum()/len(predict_test))))
  print("\n")
  print('prediction median:\t %f' %(st.median(predict_test)))

opt(200,20,4,0.5)

from sklearn import ensemble
import sys
import matplotlib.pyplot as plt
def opt(particles,iterations,dimensions,inertia):
  max_d=40
  min_d=10
  max_f=1e-5
  min_f=1e-8
  max_id=1e-4
  min_id=1e-1
  max_l=0.5
  min_l=0.1
  x=np.random.rand(particles,1)*(max_d - min_d)+min_d
  y=np.random.rand(particles,1)*(max_f - min_f)+min_f
  z=np.random.rand(particles,1)*(max_id - min_id)+min_id
  q=np.random.rand(particles,1)*(max_l - min_l)+min_l
  c=np.concatenate((x,y,z,q),axis=1)
  v=np.zeros((particles,dimensions))
  c1=2
  c2=2
  p_best=np.zeros((particles,dimensions))
  p_best_val=np.zeros(particles)+sys.maxsize
  g_best=np.zeros(dimensions)
  g_best_val=sys.maxsize
  best_iter=np.zeros(iterations)
  p_best_RGS=np.empty((particles),dtype=object)
  g_best_RGS=sys.maxsize
  #plot(c)
  from sklearn.metrics import mean_squared_error
  for i in range(iterations):
    for j in range(particles):
      rgs=ensemble.RandomForestRegressor(max_depth=c[j][0],max_features=c[j][1],min_impurity_decrease=c[j][2],
                                     min_samples_leaf=c[j][3])
      rgs.fit(train_padded,train_label_seq)
      y_predict=rgs.predict(test_padded)
      mse=mean_squared_error(testing_label_seq,y_predict)

      if(mse<p_best_val[j]):
        p_best_val[j]=mse
        p_best_RGS[j]=rgs
        p_best[j]=c[j].copy()

      aux=np.argmin(p_best_val)
      if(p_best_val[aux] < g_best_val):
        g_best_val=p_best_val[aux]
        g_best=p_best[aux].copy()
        g_best_RGS=p_best_RGS[aux]

      rand1=np.random.random()
      rand2=np.random.random()

      w=inertia
      
      v[j]=w*v[j]+c1*(p_best[j]-c[j])*rand1+c2*(g_best-c[j])*rand2
      c[j]=c[j]+v[j]

      if(c[j][3] <min_l):
        c[j][3]=min_l
      if(c[j][3] >max_l):
        c[j][3]=max_l
      if(c[j][2] <min_id):
        c[j][2]=min_id
      if(c[j][2] >max_id):
        c[j][2]=max_id
      if(c[j][1]<min_f):
        c[j][1]=min_f
      if(c[j][1]>max_f):
        c[j][1]=max_f
      if(c[j][0]<min_d):
        c[j][0]=min_d
      if(c[j][0]>max_d):
        c[j][0]=max_d
      
    best_iter[i]=g_best_val

    print('best value iteration %d = %f\n' %(i,g_best_val))

  print('group best configuration found')
  print(g_best)
  print('\n')
  print('best regressor \n')
  print(g_best_RGS)
  print('\n')

  t=range(iterations)
  plt.plot(t,best_iter,label='Fitness Value')
  plt.legend()
  plt.show()
  #plot(c)

  predict_test=g_best_RGS.predict(test_padded)
  #print(color.BOLD + "predictions with the population best value found:\n" + color.END)
  evaluate(predict_test)

def evaluate(predictions):
  from sklearn.metrics import mean_squared_error
  import statistics as st
  predict_test=predictions

  plt.plot(range(len(y_test)),y_test,label='Real')
  plt.plot(range(len(predict_test)),predict_test,label='Predicted')
  plt.legend()
  plt.show()
  
  mse=np.sqrt(mean_squared_error(y_test,predict_test))
  print("\n")
  print("Mean Squared_error: \t %f "%mse)
  print("\n")
  print('prediction average: \t %f'%((predict_test.sum()/len(predict_test))))
  print("\n")
  print('prediction median:\t %f' %(st.median(predict_test)))

opt(200,20,4,0.5)

from sklearn.linear_model import Lasso
from sklearn.ensemble import StackingRegressor

estimators = [
              ('DTREE', tree.DecisionTreeRegressor(max_depth=28.809942148548625, max_features=1e-05,
                      min_impurity_decrease=0.0001, min_samples_leaf=0.1)),
              ('svr', ensemble.RandomForestRegressor(max_depth=19.679153949842974, max_features=1e-08,
                      min_impurity_decrease=0.0001, min_samples_leaf=0.1))
               ]
reg = StackingRegressor(
   estimators=estimators,
   final_estimator=Lasso(alpha=0.00001)
)

reg.fit(train_padded,train_label_seq)
pred=reg.predict(test_padded)

from sklearn import metrics
rmse=np.sqrt(metrics.mean_squared_error(testing_label_seq,pred))
rmse



